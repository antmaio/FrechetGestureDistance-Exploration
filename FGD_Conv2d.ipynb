{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b023a70",
   "metadata": {},
   "source": [
    "# Frechet Gesture Distance From Image Data Representation and Resnet-based AutoEncoder "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f349ee19",
   "metadata": {},
   "source": [
    "## Optional: Style selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3d538d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jupyterthemes import get_themes\n",
    "import jupyterthemes as jt\n",
    "from jupyterthemes.stylefx import set_nb_theme\n",
    "# uncomment and execute line to try a new theme\n",
    "#set_nb_theme('onedork')\n",
    "#set_nb_theme('chesterish')\n",
    "#set_nb_theme('grade3')\n",
    "#set_nb_theme('oceans16')\n",
    "#set_nb_theme('solarizedl')\n",
    "#set_nb_theme('solarizedd')\n",
    "set_nb_theme('monokai')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eefe35e",
   "metadata": {},
   "source": [
    "## Import\n",
    "##### might not need all the import below "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e99331e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np \n",
    "import math\n",
    "\n",
    "import sys \n",
    "import csv \n",
    "from tqdm import tqdm\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt \n",
    "from scipy import linalg\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.preprocessing import normalize\n",
    "from skimage.util import random_noise\n",
    "\n",
    "[sys.path.append(i) for i in ['.', '..']]\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.onnx\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch_snippets import *\n",
    "#from data_loader.lmdb_data_loader import *\n",
    "\n",
    "from fastai.data.all import *\n",
    "from fastai.vision.all import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a001d65",
   "metadata": {},
   "source": [
    "## Config parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b3b228",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.name = 'gesture_autoencoder'\n",
    "\n",
    "        self.train_data_path = 'data/ted_dataset/lmdb_train'\n",
    "        self.val_data_path =  'data/ted_dataset/lmdb_val'\n",
    "        self.test_data_path =  'data/ted_dataset/lmdb_test'\n",
    "\n",
    "        self.model_save_path = 'output/train_h36m_gesture_autoencoder'\n",
    "        self.random_seed =  -1\n",
    "\n",
    "        # model params\n",
    "        self.model = 'gesture_autoencoder'\n",
    "        self.mean_dir_vec= [ 0.0154009, -0.9690125, -0.0884354, -0.0022264, -0.8655276, 0.4342174, -0.0035145, -0.8755367, -0.4121039, -0.9236511, 0.3061306, -0.0012415, -0.5155854,  0.8129665,  0.0871897, 0.2348464,  0.1846561,  0.8091402,  0.9271948,  0.2960011, -0.013189 ,  0.5233978,  0.8092403,  0.0725451, -0.2037076, 0.1924306,  0.8196916]\n",
    "        self.mean_pose= [ 0.0000306,  0.0004946, 0.0008437,  0.0033759, -0.2051629, -0.0143453,  0.0031566, -0.3054764,  0.0411491,  0.0029072, -0.4254303, -0.001311 , -0.1458413, -0.1505532, -0.0138192, -0.2835603,  0.0670333,  0.0107002, -0.2280813,  0.112117 , 0.2087789,  0.1523502, -0.1521499, -0.0161503,  0.291909 , 0.0644232,  0.0040145,  0.2452035,  0.1115339,  0.2051307]\n",
    "\n",
    "        # train params\n",
    "        self.epochs= 500\n",
    "        self.batch_size= 128\n",
    "        self.learning_rate= 0.0005\n",
    "\n",
    "        # dataset params\n",
    "        self.motion_resampling_framerate= 15\n",
    "        self.n_poses= 34\n",
    "        self.n_pre_poses= 4\n",
    "        self.subdivision_stride= 10\n",
    "        self.loader_workers= 4\n",
    "        \n",
    "args = Args()\n",
    "\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "device = torch.device(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a47071b",
   "metadata": {},
   "source": [
    "## Utils function\n",
    "#### From the author, do not edit!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef99fe26",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_vec_pairs = [(0, 1, 0.26), (1, 2, 0.18), (2, 3, 0.14), (1, 4, 0.22), (4, 5, 0.36),\n",
    "                 (5, 6, 0.33), (1, 7, 0.22), (7, 8, 0.36), (8, 9, 0.33)]  # adjacency and bone length\n",
    "\n",
    "def convert_dir_vec_to_pose(vec):\n",
    "    vec = np.array(vec)\n",
    "\n",
    "    if vec.shape[-1] != 3:\n",
    "        vec = vec.reshape(vec.shape[:-1] + (-1, 3))\n",
    "\n",
    "    if len(vec.shape) == 2:\n",
    "        joint_pos = np.zeros((10, 3))\n",
    "        for j, pair in enumerate(dir_vec_pairs):\n",
    "            joint_pos[pair[1]] = joint_pos[pair[0]] + pair[2] * vec[j]\n",
    "    elif len(vec.shape) == 3:\n",
    "        joint_pos = np.zeros((vec.shape[0], 10, 3))\n",
    "        for j, pair in enumerate(dir_vec_pairs):\n",
    "            joint_pos[:, pair[1]] = joint_pos[:, pair[0]] + pair[2] * vec[:, j]\n",
    "    elif len(vec.shape) == 4:  # (batch, seq, 9, 3)\n",
    "        joint_pos = np.zeros((vec.shape[0], vec.shape[1], 10, 3))\n",
    "        for j, pair in enumerate(dir_vec_pairs):\n",
    "            joint_pos[:, :, pair[1]] = joint_pos[:, :, pair[0]] + pair[2] * vec[:, :, j]\n",
    "    else:\n",
    "        assert False\n",
    "\n",
    "    return joint_pos\n",
    "\n",
    "\n",
    "def convert_pose_seq_to_dir_vec(pose):\n",
    "    if pose.shape[-1] != 3:\n",
    "        pose = pose.reshape(pose.shape[:-1] + (-1, 3))\n",
    "\n",
    "    if len(pose.shape) == 3:\n",
    "        dir_vec = np.zeros((pose.shape[0], len(dir_vec_pairs), 3))\n",
    "        for i, pair in enumerate(dir_vec_pairs):\n",
    "            dir_vec[:, i] = pose[:, pair[1]] - pose[:, pair[0]]\n",
    "            dir_vec[:, i, :] = normalize(dir_vec[:, i, :], axis=1)  # to unit length\n",
    "    elif len(pose.shape) == 4:  # (batch, seq, ...)\n",
    "        dir_vec = np.zeros((pose.shape[0], pose.shape[1], len(dir_vec_pairs), 3))\n",
    "        for i, pair in enumerate(dir_vec_pairs):\n",
    "            dir_vec[:, :, i] = pose[:, :, pair[1]] - pose[:, :, pair[0]]\n",
    "        for j in range(dir_vec.shape[0]):  # batch\n",
    "            for i in range(len(dir_vec_pairs)):\n",
    "                dir_vec[j, :, i, :] = normalize(dir_vec[j, :, i, :], axis=1)  # to unit length\n",
    "    else:\n",
    "        assert False\n",
    "\n",
    "    return dir_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f41c6a72",
   "metadata": {},
   "source": [
    "## Pytorch Dataset with noisy function and image transform as static methods "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c2e352",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_subject = ['S1', 'S5', 'S6', 'S7', 'S8', 'S9']\n",
    "test_subject = ['S11']\n",
    "\n",
    "class Human36M(Dataset):\n",
    "    def __init__(self, path, mean_data, is_train=True, augment=False, method=None, std=0, to_image=None):\n",
    "        n_poses = 34\n",
    "        \n",
    "        '''\n",
    "        H36M_NAMES = ['']*32\n",
    "        H36M_NAMES[0]  = 'Hip'\n",
    "        H36M_NAMES[1]  = 'RHip'\n",
    "        H36M_NAMES[2]  = 'RKnee'\n",
    "        H36M_NAMES[3]  = 'RFoot'\n",
    "        H36M_NAMES[4]  = 'RFootTip'\n",
    "        H36M_NAMES[6]  = 'LHip'\n",
    "        H36M_NAMES[7]  = 'LKnee'\n",
    "        H36M_NAMES[8]  = 'LFoot'\n",
    "        H36M_NAMES[12] = 'Spine'\n",
    "        H36M_NAMES[13] = 'Thorax'\n",
    "        H36M_NAMES[14] = 'Neck/Nose'\n",
    "        H36M_NAMES[15] = 'Head'\n",
    "        H36M_NAMES[17] = 'LShoulder'\n",
    "        H36M_NAMES[18] = 'LElbow'\n",
    "        H36M_NAMES[19] = 'LWrist'\n",
    "        H36M_NAMES[25] = 'RShoulder'\n",
    "        H36M_NAMES[26] = 'RElbow'\n",
    "        H36M_NAMES[27] = 'RWrist'\n",
    "        '''\n",
    "\n",
    "        target_joints = [1, 6, 12, 13, 14, 15, 17, 18, 19, 25, 26, 27]  # see https://github.com/kenkra/3d-pose-baseline-vmd/wiki/body\n",
    "\n",
    "        self.method = method\n",
    "        self.std = std\n",
    "        self.is_train = is_train\n",
    "        self.augment = augment\n",
    "        self.mean_data = mean_data\n",
    "        self.data = []\n",
    "\n",
    "        if is_train:\n",
    "            subjects = train_subject\n",
    "        else:\n",
    "            subjects = test_subject\n",
    "\n",
    "        # loading data and normalize\n",
    "        frame_stride = 2\n",
    "        data = np.load(path, allow_pickle=True)['positions_3d'].item()\n",
    "        for subject, actions in data.items():\n",
    "            if subject not in subjects:\n",
    "                continue\n",
    "\n",
    "            for action_name, positions in actions.items():\n",
    "                print('positions : ', positions.shape)\n",
    "                assert False\n",
    "                positions = positions[:, target_joints]\n",
    "                positions = self.normalize(positions)\n",
    "                for f in range(0, len(positions), 10):\n",
    "                    if f+n_poses*frame_stride > len(positions):\n",
    "                        break\n",
    "                    gesture = positions[f:f+n_poses*frame_stride:frame_stride]\n",
    "                    self.data.append(gesture)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        poses = self.data[index]\n",
    "        dir_vec = convert_pose_seq_to_dir_vec(poses)\n",
    "        poses = convert_dir_vec_to_pose(dir_vec)\n",
    "        \n",
    "        '''Noise for data augmentation'''\n",
    "        if self.augment:  # data augmentation by adding gaussian noises on joints coordinates            \n",
    "            rand_val = random.random()\n",
    "            if rand_val < 0.2:\n",
    "                poses = poses.copy()\n",
    "                poses += np.random.normal(0, 0.002 ** 0.5, poses.shape)\n",
    "            else:\n",
    "                poses = poses.copy()\n",
    "                poses += np.random.normal(0, 0.0001 ** 0.5, poses.shape)\n",
    "            \n",
    "        '''Noise for data alteration for FGD sensitivity assessment'''    \n",
    "        if self.method is not None: #poses alteration by adding noises of different types\n",
    "            if hasattr(self, self.method):\n",
    "                noise_function = getattr(self, self.method)\n",
    "                if len(noise_function(poses, self.std)) == 2: #temporal noise\n",
    "                    noise, r = noise_function(poses, self.std)\n",
    "                    poses[r:r+self.std] = poses[r:r+self.std] + noise\n",
    "                else:\n",
    "                    poses += noise_function(poses, self.std)\n",
    "            else:\n",
    "                raise NotImplementedError\n",
    "        \n",
    "        dir_vec = convert_pose_seq_to_dir_vec(poses)\n",
    "        dir_vec = dir_vec.reshape(dir_vec.shape[0], -1)\n",
    "        dir_vec = dir_vec - self.mean_data \n",
    "\n",
    "        poses = torch.from_numpy(poses).float()\n",
    "        dir_vec = torch.from_numpy(dir_vec).float()\n",
    "        return poses, dir_vec\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def normalize(self, data):\n",
    "\n",
    "        # pose normalization\n",
    "        for f in range(data.shape[0]):\n",
    "            data[f, :] -= data[f, 2]\n",
    "            data[f, :, (0, 1, 2)] = data[f, :, (0, 2, 1)]  # xy exchange\n",
    "            data[f, :, 1] = -data[f, :, 1]  # invert y\n",
    "\n",
    "        # frontalize based on hip joints\n",
    "        for f in range(data.shape[0]):\n",
    "            hip_vec = data[f, 1] - data[f, 0]\n",
    "            angle = np.pi - np.math.atan2(hip_vec[2], hip_vec[0])  # angles on XZ plane\n",
    "            if 180 > np.rad2deg(angle) > 0:\n",
    "                pass\n",
    "            elif 180 < np.rad2deg(angle) < 360:\n",
    "                angle = angle - np.deg2rad(360)\n",
    "\n",
    "            rot = self.rotation_matrix([0, 1, 0], angle)\n",
    "            data[f] = np.matmul(data[f], rot)\n",
    "\n",
    "        data = data[:, 2:]  # exclude hip joints\n",
    "        return data\n",
    "\n",
    "    @staticmethod\n",
    "    def to_image(data, name):\n",
    "        assert len(data.shape) == 3, \"data has no valid shape to plot\"\n",
    "        plt.figure()\n",
    "        #plt.imshow(data.numpy())\n",
    "        plt.imsave(name+'.png', data.numpy())\n",
    "        \n",
    "    @staticmethod\n",
    "    def rotation_matrix(axis, theta):\n",
    "        \"\"\"\n",
    "        Return the rotation matrix associated with counterclockwise rotation about\n",
    "        the given axis by theta radians.\n",
    "        \"\"\"\n",
    "        axis = np.asarray(axis)\n",
    "        axis = axis / math.sqrt(np.dot(axis, axis))\n",
    "        a = math.cos(theta / 2.0)\n",
    "        b, c, d = -axis * math.sin(theta / 2.0)\n",
    "        aa, bb, cc, dd = a * a, b * b, c * c, d * d\n",
    "        bc, ad, ac, ab, bd, cd = b * c, a * d, a * c, a * b, b * d, c * d\n",
    "        return np.array([[aa + bb - cc - dd, 2 * (bc + ad), 2 * (bd - ac)],\n",
    "                         [2 * (bc - ad), aa + cc - bb - dd, 2 * (cd + ab)],\n",
    "                         [2 * (bd + ac), 2 * (cd - ab), aa + dd - bb - cc]])\n",
    "    @staticmethod\n",
    "    def salt_and_pepper(data, std):\n",
    "        u = np.random.uniform(size=data[0].shape) #Applying the same noise on every frame to avoid discontinuities\n",
    "        noise = np.zeros(u.shape)\n",
    "        cond0 = np.where(u<=std/2)\n",
    "        cond1 = np.where((u>std/2) & (u<=std))\n",
    "        noise[cond0[0], cond0[1], cond0[2]] = 0.2\n",
    "        noise[cond1[0], cond1[1], cond1[2]] = -0.2\n",
    "        return noise\n",
    "    \n",
    "    @staticmethod\n",
    "    def gaussian_noise(data, std):\n",
    "        return np.random.normal(0, std, data[0].shape) #Applying the same noise on every frame to avoid discontinuities\n",
    "\n",
    "    @staticmethod\n",
    "    def temporal_noise(data, std):\n",
    "        r = np.random.randint(1, data.shape[0] - std - 1)\n",
    "        noise = np.random.normal(0, 0.003 ** 0.5, data[0].shape)\n",
    "        return noise, r\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34adbba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Normalization:\n",
    "    \n",
    "    def get_normalized_all(data):\n",
    "        #compute min and max on the dataset on all joints\n",
    "        assert data.shape[-1] == 3, \"Last channel is not xyz\"\n",
    "        bound={}\n",
    "        bound['max'] = torch.tensor((data[..., 0].max(), data[..., 1].max(), data[..., 2].max())) \n",
    "        bound['min'] = torch.tensor((data[..., 0].min(), data[..., 1].min(), data[..., 2].min()))\n",
    "\n",
    "        return (data - bound['min']) / (bound['max'] - bound['min'])\n",
    "    \n",
    "    def get_normalized_joint(data):\n",
    "        assert data.shape[-1] == 3, \"Last channel is not xyz\"\n",
    "        data_n = torch.empty(data.shape)\n",
    "        bound = {}\n",
    "        \n",
    "        for i in range(data.shape[-2]):#n joints\n",
    "            bound[f'max_{i}'] = torch.tensor((data[..., i, 0].max(), data[..., i, 1].max(), data[..., i, 2].max()))\n",
    "            bound[f'min_{i}'] = torch.tensor((data[..., i, 0].min(), data[..., i, 1].min(), data[..., i, 2].min()))\n",
    "            \n",
    "            for o in range(data.shape[-1]):\n",
    "                data_n[...,i,o] = (data[...,i,o] - bound[f'min_{i}'][o]) / (bound[f'max_{i}'][o] - bound[f'min_{i}'][o])\n",
    "            \n",
    "        return data_n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dddb8f2b",
   "metadata": {},
   "source": [
    "## Prepare datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c736e527",
   "metadata": {},
   "outputs": [],
   "source": [
    "path  = '../Gesture-Generation-from-Trimodal-Context/data/h36m/data_3d_h36m.npz'\n",
    "mean_dir_vec = np.squeeze(np.array(args.mean_dir_vec))\n",
    "\n",
    "train_dataset = Human36M(path, mean_dir_vec, is_train=True, augment=False)\n",
    "val_dataset = Human36M(path, mean_dir_vec, is_train=False, augment=False)\n",
    "#val_dataset_noisy = Human36M(path, mean_dir_vec, is_train=False, augment=False, method=method, std=std)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=args.batch_size, shuffle=False, drop_last=True)\n",
    "test_loader = DataLoader(dataset=val_dataset, batch_size=args.batch_size, shuffle=False, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc07391",
   "metadata": {},
   "source": [
    "to_image() test code : show or save an image computed from gesture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25420aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, target_vec = next(iter(test_loader))\n",
    "target_vec_ = Normalization.get_normalized_all(target_vec[0].reshape(args.n_poses, len(args.mean_dir_vec) // 3, -1)) #eliminate batch size and create 3 channels for xyz\n",
    "Human36M.to_image(torch.swapaxes(target_vec_, 0,1), 'all')\n",
    "target_vec_ = Normalization.get_normalized_joint(target_vec[0].reshape(args.n_poses, len(args.mean_dir_vec) // 3, -1)) #eliminate batch size and create 3 channels for xyz\n",
    "Human36M.to_image(torch.swapaxes(target_vec_, 0,1), 'joints')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d642f34",
   "metadata": {},
   "source": [
    "## Concatenate directory vectors\n",
    "##### Useful to create the image dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "551dcc0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_spaces = None\n",
    "dataset = train_loader\n",
    "target_poses, target_vecs = None, None\n",
    "save = False\n",
    "\n",
    "for idx, (target_pose, target_vec) in enumerate(dataset,0):\n",
    "    #Concatenation\n",
    "    if target_poses is None and target_vecs is None:\n",
    "        target_poses, target_vecs = target_pose, target_vec\n",
    "    else:\n",
    "        target_poses, target_vecs = torch.cat((target_poses, target_pose), 0), torch.cat((target_vecs, target_vec), 0)\n",
    "    if save:\n",
    "        np.savez('test_gestures_gt', data = target_poses)\n",
    "\n",
    "target_vecs = target_vecs.reshape(len(target_vecs), args.n_poses, len(args.mean_dir_vec) // 3, -1)\n",
    "print(target_vecs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3d4a66",
   "metadata": {},
   "source": [
    "## Transform the directory vectors to images\n",
    "#### MinMax normalization to build image\n",
    "###### Normalization parameters computed by joint (1 min 1 max by joint) or for all of them (1 min 1 max for all) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb7da96",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from tqdm import tqdm\n",
    "\n",
    "save_db = True\n",
    "if dataset == train_loader:\n",
    "    print('train!')\n",
    "    ds = 'train'\n",
    "elif dataset == test_loader:\n",
    "    print('valid!')\n",
    "    ds = 'valid'\n",
    "    \n",
    "if save_db: #Create the dataset folder and files of training and validation images. Take time to run... \n",
    "    \n",
    "    root = '../Gesture-Generation-from-Trimodal-Context/data/gest2im'\n",
    "    #root = os.path.join(root, ds)\n",
    "    t_all = Normalization.get_normalized_all(target_vecs)\n",
    "    t_joint = Normalization.get_normalized_joint(target_vecs)\n",
    "\n",
    "    paths = ['all', 'joint']\n",
    "    for path in paths:\n",
    "        if not os.path.exists(os.path.join(root, path, ds)):\n",
    "            os.makedirs(os.path.join(root, path, ds))\n",
    "\n",
    "    for i in tqdm(range(len(target_vecs))):\n",
    "        Human36M.to_image(torch.swapaxes(t_all[i], 0,1), os.path.join(root, paths[0], ds, f'{i}'))\n",
    "        Human36M.to_image(torch.swapaxes(t_joint[i], 0,1), os.path.join(root, paths[1], ds, f'{i}'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa230df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = '../Gesture-Generation-from-Trimodal-Context/data/gest2im'\n",
    "paths = ['all', 'joint']\n",
    "ds = 'train'\n",
    "os.path.join(root, paths[0], ds)\n",
    "len(target_vecs)\n",
    "len(t_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff48f55f",
   "metadata": {},
   "source": [
    "## Building NN Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d57c873",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UpSample(nn.Module):\n",
    "    def __init__(self,feat_in,feat_out,out_shape=None,scale=2):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(feat_in,feat_out,kernel_size=(3,3),stride=1,padding=1)\n",
    "        self.out_shape,self.scale = out_shape,scale\n",
    "        \n",
    "    \n",
    "    def forward(self,x):\n",
    "        return self.conv(\n",
    "            nn.functional.interpolate(\n",
    "                x,size=self.out_shape,scale_factor=self.scale,mode='bilinear',align_corners=True))\n",
    "    \n",
    "def get_upSamp(feat_in,feat_out, out_shape=None, scale=2, act='relu'):\n",
    "    \n",
    "    upSamp = UpSample(feat_in,feat_out,out_shape=out_shape,scale=scale).cuda()\n",
    "    \n",
    "    layer = nn.Sequential(upSamp)\n",
    "    \n",
    "    if act == 'relu':\n",
    "        act_f = nn.ReLU(inplace=True).cuda()\n",
    "        bn = nn.BatchNorm2d(feat_out).cuda()\n",
    "        layer.add_module('ReLU',act_f)\n",
    "        layer.add_module('BN',bn)\n",
    "    elif act == 'sig':\n",
    "        act_f = nn.Sigmoid()\n",
    "        layer.add_module('Sigmoid',act_f)\n",
    "    return layer\n",
    "\n",
    "def add_layer(m,feat_in,feat_out,name,out_shape=None,scale=2,act='relu'):\n",
    "    upSamp = get_upSamp(feat_in,feat_out,out_shape=out_shape,scale=scale,act=act)\n",
    "    m.add_module(name,upSamp)\n",
    "    \n",
    "def save_onnx(path, name, model):\n",
    "    \n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    \n",
    "    torch.onnx.export(model, \n",
    "                     torch.rand((64,3,28,28)).to(device), \n",
    "                     os.path.join(path, name), \n",
    "                     opset_version=11, \n",
    "                     do_constant_folding=True,\n",
    "                     input_names = ['input'], \n",
    "                     output_names = ['output'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb562ffe",
   "metadata": {},
   "source": [
    "## Fastai solution to build dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd0c291",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "splitter = FuncSplitter(lambda o: Path(o).parent.name == 'valid')\n",
    "root = Path('../Gesture-Generation-from-Trimodal-Context/data/gest2im/all')\n",
    "\n",
    "dblock = DataBlock(blocks=(ImageBlock, ImageBlock),\n",
    "                  get_items=get_image_files,\n",
    "                  get_y=lambda x: x,\n",
    "                  splitter = splitter,\n",
    "                  item_tfms=[Resize(28)])\n",
    "\n",
    "dls = dblock.dataloaders(root, batch_size = args.batch_size, num_workers=0)\n",
    "\n",
    "\n",
    "x_ = dls.one_batch()\n",
    "(x_[0] == x_[1]).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0bb500",
   "metadata": {},
   "source": [
    "## Building the resnet34 Autoencoder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e8e559",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = resnet34(pretrained=True).to(device)\n",
    "ae= nn.Sequential(*list(model.children())[:-3])\n",
    "conv = nn.Conv2d(256, 32, kernel_size =(2,2)).to(device)\n",
    "ae.add_module('CodeIn', conv)\n",
    "\n",
    "## Change size by 28x28 if resized \n",
    "#print(ae(torch.rand((64,3,9,34)).to(device)).size())\n",
    "add_layer(ae, 32, 256, 'CodeOut') #out: (2,2) \n",
    "#print(ae(torch.rand((64,3,9,34)).to(device)).size())\n",
    "add_layer(ae, 256, 128, 'Upsample0')#out: (4,4)\n",
    "#print(ae(torch.rand((64,3,9,34)).to(device)).size())\n",
    "add_layer(ae, 128, 64, 'Upsample1', out_shape=(7,7), scale=None) #out: (7,7)\n",
    "#print(ae(torch.rand((64,3,9,34)).to(device)).size())\n",
    "add_layer(ae, 64, 32, 'Upsample2') #out: out_shape\n",
    "#print(ae(torch.rand((64,3,9,34)).to(device)).size())\n",
    "add_layer(ae, 32, 3, 'Upsample3', act='sig') #out: out_shape\n",
    "#ae(torch.rand((64,3,9,34)).to(device)).size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160eea41",
   "metadata": {},
   "source": [
    "## Save model graph to onnx to visualize architecture with netron (e.g.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7794ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Export onnx to check model\n",
    "import torch.onnx\n",
    "\n",
    "path = os.path.join('..', 'Gesture-Generation-from-Trimodal-Context', 'scripts', 'model', 'onnx')\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "    \n",
    "save_onnx(path, 'resnet34.onnx', model)\n",
    "save_onnx(path, 'resnet34_AE.onnx', ae)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50cc2751",
   "metadata": {},
   "source": [
    "## AE Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb453b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "variational_encoding = False\n",
    "if not variational_encoding:\n",
    "    loss_func = F.l1_loss\n",
    "    \n",
    "learn = Learner(dls, ae, loss_func = loss_func, metrics=[mae])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2522fd6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "suggested_lr = learn.lr_find(stop_div=False,num_it=200)\n",
    "print('lr = ', suggested_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf772089",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Validation loss before fit', learn.validate()[0])\n",
    "\n",
    "cbs = []\n",
    "cbs.append(SaveModelCallback())\n",
    "cbs.append(CSVLogger(fname='models/log.csv', append=True)) \n",
    "learn.fit_one_cycle(10,lr_max=suggested_lr, cbs=cbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807575d0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "learn.show_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d6af31",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "suggested_lr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92705832",
   "metadata": {},
   "source": [
    "## Build noisy dataloaders and save it into related folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ce9536",
   "metadata": {},
   "outputs": [],
   "source": [
    "ALL_NOISE = ['gaussian_noise', 'saltandpepper_noise', 'temporal_noise']\n",
    "method = 'saltandpepper_noise'\n",
    "method_str = method.split('_')[0]\n",
    "save = False\n",
    "\n",
    "if method not in ALL_NOISE:\n",
    "    raise NotImplementedError\n",
    "    \n",
    "if method == 'gaussian_noise':\n",
    "    stds = [0.0316, 0.0447, 0.0548]\n",
    "elif method == 'saltandpepper_noise':\n",
    "    stds = [0.1,0.15,0.2] #This is not noise std!\n",
    "elif method == 'temporal_noise':\n",
    "    stds = [1,5,10] #This is not noise std!\n",
    "    \n",
    "method='gaussian_noise'\n",
    "method_str = method.split('_')[0]\n",
    "std = 0.0548\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e523103",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "path  = '../Gesture-Generation-from-Trimodal-Context/data/h36m/data_3d_h36m.npz'\n",
    "mean_dir_vec = np.squeeze(np.array(args.mean_dir_vec))\n",
    "\n",
    "val_dataset_noisy = Human36M(path, mean_dir_vec, is_train=False, augment=False, method=method,std=std)\n",
    "test_loader_noisy = DataLoader(dataset=val_dataset_noisy, batch_size=args.batch_size, shuffle=False, drop_last=True)\n",
    "\n",
    "dataset = test_loader_noisy\n",
    "target_poses, target_vecs = None, None\n",
    "save = False\n",
    "save_db = True\n",
    "\n",
    "if dataset == train_loader:\n",
    "    print('train!')\n",
    "    ds = 'train'\n",
    "elif dataset == test_loader:\n",
    "    print('valid!')\n",
    "    ds = 'valid'\n",
    "elif dataset == test_loader_noisy:\n",
    "    print('valid!')\n",
    "    ds = 'valid'\n",
    "\n",
    "for idx, (target_pose, target_vec) in enumerate(dataset,0):\n",
    "    #Concatenation\n",
    "    if target_poses is None and target_vecs is None:\n",
    "        target_poses, target_vecs = target_pose, target_vec\n",
    "    else:\n",
    "        target_poses, target_vecs = torch.cat((target_poses, target_pose), 0), torch.cat((target_vecs, target_vec), 0)\n",
    "if save:\n",
    "    np.savez('test_gestures_gt', data = target_poses)\n",
    "\n",
    "target_vecs_noisy = target_vecs.reshape(len(target_vecs), args.n_poses, len(args.mean_dir_vec) // 3, -1)\n",
    "\n",
    "if save_db: #Create the dataset folder and files of training and validation images. Take time to run... \n",
    "    \n",
    "    root = '../Gesture-Generation-from-Trimodal-Context/data/gest2im'\n",
    "    #root = os.path.join(root, ds)\n",
    "    t_all = Normalization.get_normalized_all(target_vecs_noisy)\n",
    "    t_joint = Normalization.get_normalized_joint(target_vecs_noisy)\n",
    "\n",
    "    paths = [f'all_{method_str}_{std}', f'joint_{method_str}_{std}']\n",
    "    for path in paths:\n",
    "        if not os.path.exists(os.path.join(root, path, ds)):\n",
    "            os.makedirs(os.path.join(root, path, ds))\n",
    "\n",
    "    for i in tqdm(range(len(target_vecs))):\n",
    "        Human36M.to_image(torch.swapaxes(t_all[i], 0,1), os.path.join(root, paths[0], ds, f'{i}'))\n",
    "        Human36M.to_image(torch.swapaxes(t_joint[i], 0,1), os.path.join(root, paths[1], ds, f'{i}'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f455eaa5",
   "metadata": {},
   "source": [
    "## Plotting the results evaluate by Conv2d.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255bfdb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "method_str = 'gaussian'\n",
    "n_poses = 64\n",
    "\n",
    "if n_poses == 34:\n",
    "    if method_str == 'gaussian':\n",
    "        zeta = np.array([0.001,0.002,0.003,0.01,0.1])\n",
    "        mean = np.array([0.08240203,0.22613631,0.39619036,1.67765289,6.87118926])\n",
    "        std = np.array([0.00132868, 0.00347157, 0.00530228, 0.01495325, 0.03741726])\n",
    "    elif method_str == 'saltandpepper':\n",
    "        zeta = np.array([0.1,0.15,0.2,0.5, 0.75])\n",
    "        mean = np.array([0.4685614,0.81640321,1.18324545,3.17272931,4.26709501])\n",
    "        std = np.array([0.0089291, 0.01159924, 0.01342414, 0.02509215, 0.03474339])\n",
    "    elif method_str == 'temporal':\n",
    "        zeta = np.array([1,5,10,20])\n",
    "        mean = np.array([0.0053349,0.02376117,0.06705022,0.18552624])\n",
    "        std = np.array([0.00010305, 0.00040202, 0.00105999, 0.00263833])\n",
    "        \n",
    "elif n_poses == 18:\n",
    "    if method_str == 'gaussian':\n",
    "        zeta = np.array([0.001,0.002,0.003,0.01,0.1])\n",
    "        mean = np.array([0.07783876,0.19267581,0.322209097,1.39058165,5.90966723])\n",
    "        std = np.array([0.0148825, 0.02148662, 0.02356199, 0.017614, 0.03530067])\n",
    "    elif method_str == 'saltandpepper':\n",
    "        zeta = np.array([0.1,0.15,0.2,0.5, 0.75])\n",
    "        mean = np.array([0.36634205,0.6579607,0.9713845,2.69054704,3.65799937])\n",
    "        std = np.array([0.00910893, 0.01183706, 0.01588302, 0.02437597, 0.0286117])\n",
    "    elif method_str == 'temporal':\n",
    "        zeta = np.array([1,5,10,15])\n",
    "        mean = np.array([0.03455983,0.09343792,0.16167952,0.28593734])\n",
    "        std = np.array([0.00292365, 0.00479823, 0.00764541, 0.00876047])\n",
    "        \n",
    "elif n_poses == 64:\n",
    "    if method_str == 'gaussian':\n",
    "        zeta = np.array([0.001,0.002,0.003,0.01,0.1])\n",
    "        mean = np.array([0.06762687,0.15387838,0.24231669,1.09294023,5.176])\n",
    "        std = np.array([0.0312, 0.0416557, 0.0283022, 0.02501722, 0.03613215])\n",
    "    elif method_str == 'saltandpepper':\n",
    "        zeta = np.array([0.1,0.15,0.2,0.5, 0.75])\n",
    "        mean = np.array([0.36149179,0.58324156,0.81554461 ,2.20881879,3.0648695])\n",
    "        std = np.array([0.07254474, 0.06982494, 0.0617641, 0.02089, 0.02318809])\n",
    "    elif method_str == 'temporal':\n",
    "        zeta = np.array([1,5,10,15, 32])\n",
    "        mean = np.array([0.00155409,0.00613046,0.01181387,0.02123891, 0.07215359])\n",
    "        std = np.array([0.000102, 0.00015345, 0.00026414, 0.00041197, 0.00390443])\n",
    "    \n",
    "save = False\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_ylabel('Fréchet Gesture Distance')\n",
    "x = np.arange(len(mean))\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(zeta)\n",
    "ax.set_title(f'Fréchet Gesture Distance evolution with $\\zeta$ ({method_str} noise)')\n",
    "ax.bar(x, mean, yerr=std, align='center', alpha=0.5, ecolor='black', capsize=10)\n",
    "ax.yaxis.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "if save:\n",
    "    plt.savefig(f'../Gesture-Generation-from-Trimodal-Context/figures/{args.n_poses} frames/fgd_{method_str}_{n}_s2_onenoise.png')\n",
    "else:\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e3770fd4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5,)\n",
      "(5,)\n",
      "(5,)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "\n",
    "mode = 'motion' #'motion' or ' ' (gesture)\n",
    "\n",
    "noise = 'gaussian_noise'\n",
    "method = f'{noise}_motion' if mode == 'motion' else noise\n",
    "\n",
    "#noises = ['gaussian_noise', 'saltandpepper_noise', 'temporal_noise']\n",
    "#methods = ['gaussian_noise_motion', 'saltandpepper_motion', 'temporal_noise_motion']\n",
    "\n",
    "if 'gaussian_noise' in method:\n",
    "   zeta = np.array([0.001,0.002,0.003,0.01,0.1])\n",
    "elif 'saltandpepper_noise' in method:\n",
    "   zeta = np.array([0.1,0.15,0.2,0.5, 0.75])\n",
    "elif 'temporal_noise' in method:\n",
    "   zeta = np.array([1,5,10,15,32])\n",
    "\n",
    "xindex = np.array([0,1,2,4,5,6,8,9,10,12,13,14,16,17,18])\n",
    "colors = ['red', 'green', 'blue']\n",
    "labels = ['18 frames' , '34 frames', '64 frames']\n",
    "#fig, axes = plt.subplots(3, sharex=True, sharey=True)\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_ylim([0, 5])\n",
    "\n",
    "#for idx,ax in enumerate(axes):\n",
    "files = [x for x in os.listdir('./evaluation/') if method in x]\n",
    "means, stds = np.zeros((len(files), len(zeta))), np.zeros((len(files),  len(zeta)))\n",
    "for i,f in enumerate(files):\n",
    "    data = np.load('./evaluation/' + f)\n",
    "    print(data['mean'].shape)\n",
    "    if len(data['mean'])!=len(zeta):\n",
    "        mean_ = np.concatenate((data['mean'], np.array([0])))\n",
    "        std_ = np.concatenate((data['std'], np.array([0])))\n",
    "    else:\n",
    "        mean_ = data['mean']\n",
    "        std_ = data['std']\n",
    "    means[i], stds[i] = mean_, std_\n",
    "\n",
    "#ax.set_ylabel(f'Fréchet {mode} Distance')\n",
    "for f in range(means.shape[-1]):\n",
    "   imin = f*3\n",
    "   imax = imin + 3\n",
    "   if f < means.shape[-1]-1:\n",
    "       ax.bar(xindex[imin:imax], means[:,f], color = colors, yerr=stds[:,f], align='center', alpha=0.75, ecolor='black', capsize=5)#, label='a')\n",
    "else:\n",
    "    for j in range(len(colors)):\n",
    "        ax.bar(xindex[imin:imax][j], means[j,f], color = colors[j], yerr=stds[j,f], align='center', alpha=0.75, ecolor='black', capsize=5, label=labels[j])\n",
    "    xt = np.arange(1, 20, 4)\n",
    "ax.set_xticks(xt)\n",
    "ax.set_xticklabels(zeta)\n",
    "method_str = method if mode == 'gesture' else method[:-7]\n",
    "#ax.set_title(f'Fréchet Gesture Distance evolution with $\\zeta$ ({method_str} noise)')\n",
    "ax.grid(linewidth=0.5)\n",
    "ax.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b51715",
   "metadata": {},
   "source": [
    "### Add noise to an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7f2ff501",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  ...\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]]\n",
      "\n",
      " [[255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  ...\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]]\n",
      "\n",
      " [[255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  ...\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  ...\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]]\n",
      "\n",
      " [[255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  ...\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]]\n",
      "\n",
      " [[255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  ...\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]]]\n",
      "(437, 437, 3)\n",
      "32.99369592244728\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2658786b848>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "%matplotlib inline\n",
    "#The line above is necesary to show Matplotlib's plots inside a Jupyter Notebook\n",
    "\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "'''\n",
    "Parameters\n",
    "----------\n",
    "image : ndarray\n",
    "    Input image data. Will be converted to float.\n",
    "mode : str\n",
    "    One of the following strings, selecting the type of noise to add:\n",
    "\n",
    "    'gauss'     Gaussian-distributed additive noise.\n",
    "    'poisson'   Poisson-distributed noise generated from the data.\n",
    "    's&p'       Replaces random pixels with 0 or 1.\n",
    "    'speckle'   Multiplicative noise using out = image + n*image,where\n",
    "                n is uniform noise with specified mean & variance.\n",
    "'''\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "def noisy(noise_typ,image):\n",
    "    if noise_typ == \"gauss\":\n",
    "        row,col,ch= image.shape\n",
    "        mean = 0\n",
    "        var = 0.00001\n",
    "        sigma = var**0.5\n",
    "        gauss = np.random.normal(mean,sigma,(row,col,ch))\n",
    "        gauss = gauss.reshape(row,col,ch)\n",
    "        noisy = image + gauss\n",
    "        return noisy\n",
    "    elif noise_typ == \"s&p\":\n",
    "        row,col,ch = image.shape\n",
    "        s_vs_p = 0.5\n",
    "        amount = 0.004\n",
    "        out = np.copy(image)\n",
    "        # Salt mode\n",
    "        num_salt = np.ceil(amount * image.size * s_vs_p)\n",
    "        coords = [np.random.randint(0, i - 1, int(num_salt))\n",
    "              for i in image.shape]\n",
    "        out[coords] = 1\n",
    "\n",
    "        # Pepper mode\n",
    "        num_pepper = np.ceil(amount* image.size * (1. - s_vs_p))\n",
    "        coords = [np.random.randint(0, i - 1, int(num_pepper))\n",
    "              for i in image.shape]\n",
    "        out[coords] = 0\n",
    "        return out\n",
    "    elif noise_typ == \"poisson\":\n",
    "        vals = len(np.unique(image))\n",
    "        vals = 2 ** np.ceil(np.log2(vals))\n",
    "        noisy = np.random.poisson(image * vals) / float(vals)\n",
    "        return noisy\n",
    "    elif noise_typ ==\"speckle\":\n",
    "        row,col,ch = image.shape\n",
    "        gauss = np.random.randn(row,col,ch)\n",
    "        gauss = gauss.reshape(row,col,ch)        \n",
    "        noisy = image + image * gauss\n",
    "        return noisy\n",
    "    \n",
    "\n",
    "image = cv2.imread('../../SIGRAPPH2022/clean_image.png') # Only for grayscale image\n",
    "print(image)\n",
    "noise_img = noisy(\"gauss\",image)\n",
    "print(noise_img.shape)\n",
    "print(noise_img.min())\n",
    "plt.imshow(noise_img)\n",
    "#cv2.imshow('sp_noise', noise_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bbb0cce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
